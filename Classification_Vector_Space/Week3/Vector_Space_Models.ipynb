{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector space models capture semantic meaning and relationships between words. You'll learn how to create word vectors that capture dependencies between words, then visualize their relationships in two dimensions using PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space Models\n",
    "\n",
    "Vector spaces are fundamental in many applications in NLP. If you were to represent a word, document, tweet, or any form of text, you will probably be encoding it as a vector. These vectors are important in tasks like information extraction, machine translation, and chatbots. Vector spaces could also be used to help you identify relationships between words as follows: \n",
    "\n",
    "<img src=\"../../img/vector_space_model.PNG\" alt=\"vecto\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "The famous quote by Firth says, \"You shall know a word by the company it keeps\". When learning these vectors, you usually make use of the neighboring words to extract meaning and information about the center word. If you were to cluster these vectors together, as you will see later in this specialization, you will see that adjectives, nouns, verbs, etc. tend to be near one another. Another cool fact, is that synonyms and antonyms are also very close to one another. This is because you can easily interchange them in a sentence and they tend to have similar neighboring words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word by Word and Word by Doc.\n",
    "\n",
    "**Word by Word Design**\n",
    "We'll begin by examining the word-by-word approach. Suppose you want to devise a vector representing a specific word. One approach could involve constructing a matrix where each row and column represents a word in your vocabulary. As you go through a document, observe how frequently each word appears adjacent to another word and record these occurrences in the matrix. In the video, I discussed a parameter $( K $). Consider $( K $) as the bandwidth that determines if two words are considered adjacent.\n",
    "\n",
    "<img src=\"../../img/word1.PNG\" alt=\"vecto\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "In the given example, notice how we track the frequency of words appearing together within a specific distance $( k $). Ultimately, you can represent word data as a vector $( v = [2, 1, 1, 0] $).\n",
    "\n",
    "**Word by Document Design**\n",
    "\n",
    "This concept can also be extended to correlate words with documents. Here, rows represent individual words while columns represent different documents. The values in the matrix indicate the frequency of each word's occurrence within a particular document.\n",
    "\n",
    "<img src=\"../../img/word2.PNG\" alt=\"vecto\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "The entertainment category can be depicted as a vector $( v = [500, 7000] $). Following this representation, you can easily compare different categories by creating a straightforward plot.\n",
    "\n",
    "<img src=\"../../img/word3.PNG\" alt=\"vecto\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "Later this week, you will see how you can use the angle between two vectors to measure similarity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
